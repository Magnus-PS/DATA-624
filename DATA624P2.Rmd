 ---
title: "DATA624HW8"
author: "Gabriel Abreu, Magnus Skonberg"
date: "`r Sys.Date()`"
output: 
 html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: flatly
    highlight: tango
---

```{r setup, include = FALSE }

# Libraries and Options

knitr::opts_chunk$set(echo = F, warning = F, message = F, eval = T, 
                      fig.height = 5, fig.width = 10) 

library(RCurl)
library(dplyr)
library(forecast)
library(ggplot2)
library(knitr)
library(inspectdf)
library(corrplot)
library(tidyverse)
library(VIM)
library(tidyr)
library(kableExtra)
library(mice)
library(Boruta)
library(randomForest)

# Comment out below if / when we need

# library(MASS)
# library(mlbench)
  library(caret)
# library(earth)
# library(nnet)
# library(car)
# library(AER)
# library(faraway)
# library(vcd)
# library(caret)
# library(boot)
# library(pscl) #predict.zeroinfl

```

```{r custom-functions, include=FALSE}

options(scipen = 9)
set.seed(123)

boxplot_depend_vs_independ <- function(df_train, target_name) {

  train_int_names <- df_train %>% select_if(is.numeric)
  int_names <- names(train_int_names)
  myGlist <- vector('list', length(int_names))
  names(myGlist) <- int_names
  
  for (i in int_names) {       
 
   myGlist[[i]] <- 
       ggplot(df_train, aes_string(x = target_name, y = i)) + 
        geom_boxplot(color = 'steelblue', outlier.color = 'firebrick', 
                     outlier.alpha = 0.35) +
        labs(title = paste0(i,' vs target'), y = i, x= 'target') +
        theme_minimal() + 
        theme(
          plot.title = element_text(hjust = 0.45),
          panel.grid.major.y =  element_line(color = "grey", 
                                             linetype = "dashed"),
          panel.grid.major.x = element_blank(),
          panel.grid.minor.y = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.ticks.x = element_line(color = "grey")
        )
       
      }

    myGlist <- within(myGlist, rm(target_name))
    gridExtra::grid.arrange(grobs = myGlist, ncol = 4)
}

plot_corr_matrix <- function(dataframe, significance_threshold){
  title <- paste0('Correlation Matrix for significance > ',
                  significance_threshold)
  
  df_cor <- dataframe %>% mutate_if(is.character, as.factor)
  
  df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)
  #run a correlation and drop the insignificant ones
  corr <- cor(df_cor)
  #prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  corr[corr == 1] <- NA 
  #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr) 
  #select significant values  
  corr <- subset(corr, abs(Freq) > significance_threshold) 
  #sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),] 
  #print table
  # print(corr)
  #turn corr back into matrix in order to plot with corrplot
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
  
  #plot correlations visually
  corrplot(mtx_corr,
           title=title,
           mar=c(0,0,1,0),
           method='color', 
           tl.col="black", 
           na.label= " ",
           addCoef.col = 'black',
           number.cex = .9)
}
```

# Background

The purpose of our second project is to work as a team to apply concepts from the 2nd half of our Predictive Analytics course to a beverage data set. More specifically, to explore the data, determine whether we might use a linear regression, non-linear regression or tree-based model, to then build, compare, select our optimal model, and support why we made the selection that we did.

## ABC Beverage Spec
 
**This is role playing. I am your new boss. I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me. My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.**

**Please use the historical data set I am providing. Build and report the factors in BOTH a technical and non-technical report. I like to use Word and Excel. Please provide your non-technical report in a  business friendly readable document and your predictions in an Excel readable format. The technical report should show clearly the models you tested and how you selected your final approach.**

```{r import-data, message=FALSE, warning=FALSE, eval=TRUE}

# read in data (as CSVs)
train <- read.csv("https://raw.githubusercontent.com/Magnus-PS/DATA-624/main/624P2TrainData.csv")
test <- read.csv("https://raw.githubusercontent.com/Magnus-PS/DATA-624/main/624P2EvalData.csv")

#rename i..Brand.Code
names(train)[names(train) == "ï..Brand.Code"] <- "Brand.Code"
names(test)[names(test) == "ï..Brand.Code"] <- "Brand.Code"

# label datasets
train$Dataset <- 'train'
test$Dataset <- 'test'

# merge datasets
final_df <- rbind(train, test)

```

## Approach

We will explore, analyze and model a data set containing approximately 2,600 training and 300 test records representing various brands of a beverage. The variables appear to be related to beverage chemical and manufacturing properties as well as the brand of beverage created. 

The response variable is `PH` which represents the pH level of each beverage. We'll generate multiple models and then select the best performing model for our training data prior to casting predictions for our test data's `PH` variable (which is currently all NAs).

We'll follow the general, high-level approach that is common in Data Science:

* Exploratory Data Analysis (EDA),
* Data Preparation,
* Model Building, and
* Model Selection

Prior to casting our predictions.

---

# Exploratory Data Analysis (EDA)

The purpose of exploring our data is to enhance the precision of the questions we’re asking while building a firm understanding of the data. The aim is to familiarize ourselves with the data's structure, the status of missing values, outliers, predictive strength, and correlation to then take the actions necessary to prepare our data for model building. 

We get to know the structure, value ranges, proportion of missing values, distribution of values and correlation with the target variable. After this point, we should have enough insight to prepare our data and build our model(s).

## Data Structure

We utilize the built-in `glimpse` method to gain insight into dimensions, variable characteristics, and value ranges for our training data:

```{r glimpse-data}

glimpse(train)

```

From above, we gather that:

* we're dealing with training data with 33 variables (discluding our addition of a "dataset" variable) and 2571 observations,
* a response variable `pH` of type double,
* a `Brand Code` categorical variable,
* remaining dependent variables of type int and double,
* variables that may be useless (ie. `Hyd.Pressure1` with all 0s),
* a significant difference in the scale of many features (ie. `Filler.Speed` v. `Oxygen.Filler`), and
* a presence of missing / NA values.

Let's get a high level look at our distributions: 

```{r}

summary(train)

```

We note presence of negative values (ie. `Pressure.Vacuum`), difference of scales and distribution, and confirm the presence of NA values (including 212 values in `MFR`) across numerous variables.

## Missing Values

Next, we investigate missing values in greater depth:

```{r aggr-plots1, results=F, fig.height=8, fig.width=15}

VIM::aggr(final_df %>% filter(Dataset == 'train'), col=c('green','red'), numbers=T, sortVars=T,
          cex.axis = .7,
          ylab=c("Proportion of Data", "Combinations and Percentiles"))

```

From the proportion chart above on the left we can see that:

* `MFR` has ~8% missing values (the most of any variable),
* a significant proportion of variables have 1-2% missing values (ie. `Filler.Speed`),
* a significant proportion of variables have less than 1% missing data (ie. `PSC.Fill`), and
* 4 variables (ie. `Brand.Code`) have no missing data.

When we shift our attention to the Combinations and Percentiles chart (on the right), it appears that while there may be instances of related patterns between variables (ie. `MFR` and `Fill.Pressure`), the data is primarily missing at random.

## Data Type Conversions

We're primarily dealing with numeric variables but in order to properly utilize our categorical variable, we convert it to type factor. We re-label empty strings as "Unlabelled" and then convert our `Brand.Code` variable to be of type factor:

```{r}

#Relabel missing brand codes to be "Unlabelled"
final_df$Brand.Code[final_df$Brand.Code == ""] <- NA
final_df$Brand.Code <- final_df$Brand.Code %>% tidyr::replace_na('Unlabelled')

#convert brand code to factor
final_df  <- final_df %>% dplyr::mutate(Brand.Code = factor(Brand.Code, levels = c('Unlabelled', 'A','B','C','D'), ordered = TRUE))

```

## Numeric Variable Distributions

Earlier we'd noted the vast difference in ranges between numeric variables. To explore this point further and gain greater insight as to the distribution of each variable, we utilize inspectdf's `inspect_num()` function:

```{r, fig.height=20, fig.width=15}

#Variable distributions
inspectdf::inspect_num(final_df %>% filter(Dataset == 'train')) %>% 
  show_plot()

```

We observe a range of distributions:

* `Carb.Pressure`, `Carb.Temp`, `Carb,Volume`, `Fill.Ounces`, `PC.Volume`, `PH`, `Pressure.Vacuum`, `PSC`, `PSC.Fill`, and `Temperature` have relatively normal spreads where just a few of the variables are skewed and would require centering / normalization,
* `Alch.Rel`, `Bowl.Setpoint`, and `Pressure.Setpoint` appear to resemble distinct numerical variables with distributions centered about a few values, and
* remaining distributions are non-normal and in need of scaling and centering.

From above, there may be a place for introducing dummy / flag variables (ie. `Alch.Rel` = 0.6) and that
centering and scaling will prove essential prior to model-building. Fortunately, R has built in functions to center and scale automatically and en masse.

## Categoric Contingency Table

Next, we create a contingency table for our categorical variable to better understand the relationship between `Brand.Code` and `PH`:

```{r}

table(final_df$Brand.Code, final_df$PH)

```

From above we might extend that:

* B is the most popular beverage brand,
* that the distributons are quite sparse outside of the range of 8.3 thru 8.8, and 
* the brand of beverage appears to have some correlation to PH level.

## Boxplots

With a basic understanding of the distribution of each of our features, we turn to the boxplot of each numeric distribution in order to visualize the magnitude of outliers:

```{r boxplot2, fig.height=20, fig.width=12} 

# Utilize customized box plot generation function
df_train <- final_df %>% filter(Dataset == 'train')
boxplot_depend_vs_independ(df_train, df_train$PH)

```

What we gather from the output is:

* our response variable (`PH`) has a range of ~8.4-8.7 with outlier values > 9.0 and ~ 8.0,
* `Mnf.Flow`, `Hyd.Pressure2`, `Hyd.Pressure3` `Usage.cont`, `Carb.Flow`, `Density`, `Balling`, `Bowl.Setpoint`, `Pressure.Setpoint`, and `Balling.Lvl` have **no outliers**,
* `Carb.Volume`, `Carb.Pressure`, `PSC.CO2`, `Hyd.Pressure1`, `Filler.Level`, `Pressure.Vacuum`, and `Alch.Rel` have **minimal outliers**, and
* remaining (15) variables have a **significant presence of outliers**.

As such, outlier-handling will be essential in properly predicting our continuous, numeric response variable (`PH`).

## Correlation Matrix

Having reviewed the structure, distributions, and presence of outliers for our variables, we turn our attention to exploring the relationship these variables have with one another via **correlation matrix**. We consider only variables with a correlation significant > 0.1 in our plot:

```{r correlation-matrix , warning=FALSE}

#Utilize custom-built correlation matrix generation function
plot_corr_matrix(final_df %>% filter(Dataset == 'train'), -1)

```

It does not appear that multicollinearity is a concern of ours. From this we might extend that feature exclusion based on multicollinearity will not be a concern.

## Variable Importance

As a final step in the exploration of our data, we shoot to understand which variables have the strongest v. weakest relationship with `PH` level. This information may prove useful later during feature exclusion / selection:

```{r boruta, comment=FALSE, warning=FALSE, message=FALSE, fig.height = 8, fig.width = 10}

# Perform Boruta search
boruta_output <- Boruta(PH ~ ., data=na.omit(train), doTrace=0, maxRuns = 1000)

# Get significant variables including tentatives
boruta_signif <- getSelectedAttributes(boruta_output, withTentative = TRUE)
#print(boruta_signif)

# Plot variable importance
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")

```

Our Boruta output indicates that 4 of our features may be insignificant (`PSC.CO2`, `PSC`, `PSC.Fill` and `Carb.Temp`) while the remainder appear to be significant. With `Mnf.Flow` carrying the greatest variable importance (by a narrow margin).

## EDA Summary

The training dataset has 33 variables and 2571 observations. The remediation actions became apparent over the course of our exploratory data analysis:

* There are numerous features with a weak relationship to `PH`. While on the surface we might consider dropping these variables (ie. `PSC.CO2`), first we should see whether their importance increases once we consider other features during our regression / model-building.
* Multicollinearity does not appear to be a concern.
* There are outliers in the majority of our features that will need to be addressed.
* Imputing missing values will be an important factor in our modeling. From our analysis, KNN may be a logical approach since beverages of similar compositions (PH levels) should have similar values.
* Centering and scaling will likely be essential prior to model-building and especially depending on the approach we elect for predicting our continuous, numeric response variable (`PH`).

The recommendations above provide a “starting line” for our data preparation. Being that, the majority of our features appear to have a relatively strong relationship with `PH`, we'd anticipate being able to pull much signal from the variables at hand and developing a strong model.

---

# Data Prep

With insights from exploratory data analysis, we set out to prepare our data for modeling. Our aim is to "optimize" our data for modeling. We'll impute missing values, identify and rectify outliers, and then proceed to model building.

## Missing Value Imputation

Since there are many variables at play, we took a nearest neighbor approach (consider the 5 nearest neighbors) for imputing values in columns where missing values were identified prior to verifying the presence of 0 NA values in our output:

```{r imputation}

#kNN imputation
#summary(train) #identify variables with missing values

final_df <- VIM::kNN(final_df, variable = c('Carb.Volume','Fill.Ounces', 'PC.Volume', 'Carb.Pressure',  'Carb.Temp', 'PSC', 'PSC.Fill', 'PSC.CO2', 'Mnf.Flow', 'Carb.Pressure1', 'Fill.Pressure', 'Hyd.Pressure1', 'Hyd.Pressure2', 'Hyd.Pressure3', 'Hyd.Pressure4', 'Filler.Level', 'Filler.Speed', 'Temperature', 'Usage.cont', 'Carb.Flow', 'Density', 'MFR', 'Balling', 'Pressure.Vacuum', 'Oxygen.Filler', 'Bowl.Setpoint', 'Pressure.Setpoint', 'Air.Pressurer', 'Alch.Rel', 'Carb.Rel', 'Balling.Lvl'), k = 5)

# verify successful imputation
colSums(is.na(final_df))

```

We confirm successful imputation via presence of all 0s when we search our variables for NA values. *Note: the `PH` column has 271 NA values because we imputed training and test data in one fell swoop and didn't touch this column since this is what we're later predicting.*
-------------------------------------------------------------------------------------------------
## Outlier Handling

With missing values imputed, we proceed to handle those that can skew the predictive capabilities of our models, the outliers.

**LEFT OFF HERE**

NOTE TO GABE: I didn't go beyond this point

for outlier handling we could build a basic linear model and use Cook's distance ...

```{r}

```

**Discussion Topic**

NOTE TO MAGNUS: I found a bunch of different ways to handle outliers. Quesion is do we need to handle them? Random Forest can handle the outliers. We can also do a BoxCox transformation on the data or a YeoJohnson using the PreProcess function.

The Training set has NA values for PH, which messes things up later on, so I think we need to impute PH values. The imputation using KNN caused final_df to have a total of 64 columns...so I did some cleaning below. We can discuss if you don't like the approach. 



-------------------------------------------------------------------------------------------------

## Normalization 

We can utilize the preProcess function from the caret package to center and scale the variables. This will optimize the data set for the different models we're going to build. It is important to note that Random Forest does not need the data to be normalized. By scaling our data, it can smooth any nonlinear relationships in the model. If there are nonlinear relationships, by transforming the data, these nonlinearities are not going to be reflected in the estimates. 

However, it is important to normalize data for regression algorithms, as well as neural networks. Since we are going to implement regression and random forest models, it is best to create 2 data sets, one normalized and one that is not. 

```{r, echo=FALSE}
df_train2 <- df_train

df_train2 <- df_train2 %>% select(c(-"Brand.Code", -"Dataset", -"PH"))

```


```{r}
set.seed(150)
Processed <- preProcess(df_train2, 
                   method = c("knnImpute", "center", "scale"))
complete_ph <- predict(Processed, df_train2)
complete_ph$PH = df_train$PH

```

```{r}
complete_ph <- complete_ph %>% na.omit()
```

```{r, echo=FALSE}

phframe <- complete_ph[,-c(nearZeroVar(complete_ph))]
corr_data <- findCorrelation(cor(phframe), cutoff = 0.90)
phframe <- phframe[, -corr_data]

```

Split Normalized Data Into Training/Testing Set

```{r, echo=FALSE}
set.seed(150)
predictors <- subset(phframe, select = -PH)
PH <- subset(phframe, select="PH")

initsplit2 <- createDataPartition(PH$PH, p=0.8, list=FALSE)


#Create Training Data to tune the model
X.train.cs <- predictors[initsplit2,]
Y.train.cs <- PH[initsplit2,]


#Create testing data to evaluate the model
X.test.cs <- predictors[-initsplit2,]
Y.test.cs <- PH[-initsplit2,]

```

# Model Building

## Linear Regression

### Multi-linear regression

```{r}

linear_model <- train(X.train.cs, Y.train.cs,
                      method='lm',
                      trControl=trainControl(method = "cv"))
```


### AIC optimized

```{r}
aic_model <- train(X.train.cs, Y.train.cs,
                      method='lmStepAIC',
                      trControl=trainControl(method = "cv"))

```


### Partial Least Squares

```{r}
pls_model <- train(X.train.cs, Y.train.cs,
                      method='pls',
                      metric='Rsquared',
                      tuneLength=10,
                      trControl=trainControl(method = "cv"))

```

Linear Model Predictions:

```{r}
set.seed(150)


lmPred <- predict(linear_model, newdata = X.test.cs)
lmResample <- postResample(pred=lmPred, obs = Y.test.cs)

aicPred <-predict(aic_model, newdata=X.test.cs)
aicResample <- postResample(pred=aicPred, obs=Y.test.cs)

plsPred <-predict(pls_model, newdata=X.test.cs)
plsReSample <- postResample(pred=plsPred, obs = Y.test.cs)

```

```{r}
display <- rbind(
"Linear Regression" = lmResample,
"Stepwise AIC" = aicResample,
"Partial Least Sqaures" = plsReSample
)


display %>% kable() %>% kable_paper()


```

Importance of Variables (Best Non-linear Model):

Using Rsquared and MAE as the principal criteria for selecting the best model, the linear model produces the best metrics. However, the Rsquared value is very low. All of the linear models produced very similar results, suggesting that the relationship between our target variable and the predictors is not linear. 

Let's investigate the top ten influential predictors. 

```{r, echo=FALSE}
plot(varImp(linear_model), top = 10)
```

We see that `Mnf.Flow`, `Temperature`, `Carb.Pressure1`, `Usage.cont`, `Hyd.Pressure2` and `Filler.Level` contribute the most to the pH levels according to the linear regression model. The linear model places heavy emphasis on the `Mnf.Flow` and equal importance to `Temperature` and `Carb.Pressure1`.


## Non-Linear Regression

### MARS

```{r}

marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)


set.seed(100)
marsM <- train(X.train.cs, Y.train.cs,
                    method = "earth",
                    tuneGrid = marsGrid,
                    trControl = trainControl(method = "cv"))
```

### Support Vector Machine

```{r}
supModel <- train(X.train.cs, Y.train.cs,
 method = "svmRadial",
 tuneLength = 14,
 trControl = trainControl(method = "cv"))

```

### KNN

```{r}
knnModel <- train(x = X.train.cs,
                  y = Y.train.cs,
                  method = "knn",
                  tuneLength = 10)

```

Non-Linear Model Predictions:

```{r}
set.seed(150)


marsPred <- predict(marsM, newdata = X.test.cs)
marsResample <- postResample(pred=marsPred, obs = Y.test.cs)

svmPred <-predict(supModel, newdata=X.test.cs)
svmResample <- postResample(pred=svmPred, obs=Y.test.cs)

knnPred <-predict(knnModel, newdata=X.test.cs)
knnResample <- postResample(pred=knnPred, obs=Y.test.cs)



```

```{r}
display <- rbind(
"MARS" = marsResample,
"Support Vector Machine" = svmResample,
"KNN" = knnResample
)


display %>% kable() %>% kable_paper()


```

Importance of Variables (Best Non-linear Model):

Using Rsquared and MAE as the principal criteria for selecting the best model, the support vector machine produces the best metrics. However, the Rsquared value is still slightly below 0.50, so it is an overall poor model. Despite, the low Rsquared we can evaluate the most influential variables. 

```{r, echo=FALSE}
plot(varImp(supModel), top = 10)
```

The non-linear model places high importance of `Mnf.Flow` just as the linear model, but there is a difference in the other variables. `Usage.cont` and `Filler.Level` are secondary and tertiary top contributors. 


## Trees

The tree models are going to use data that is not normalized but still require imputation, removal of highly correlated data, and variables that contain near zero variance. 

```{r, echo=FALSE}
df_train3 <- df_train

df_train3 <- df_train3 %>% select(c(-"Brand.Code", -"Dataset", -"PH"))

```


```{r, echo=FALSE}
set.seed(150)
Processed2 <- preProcess(df_train3, 
                   method = c("knnImpute"))
comPH <- predict(Processed2, df_train3)
comPH$PH = df_train$PH

```

```{r, echo=FALSE}
comPH <- complete_ph %>% na.omit()
```

```{r, echo=FALSE}

phframe2 <- comPH[,-c(nearZeroVar(comPH))]
corr_data <- findCorrelation(cor(phframe2), cutoff = 0.90)
phframe2 <- phframe[, -corr_data]

```

```{r, echo=FALSE}
set.seed(150)
predictors2 <- subset(phframe2, select = -PH)
PH2 <- subset(phframe2, select="PH")

initsplit3 <- createDataPartition(PH2$PH, p=0.8, list=FALSE)


#Create Training Data to tune the model
X.train <- predictors[initsplit3,]
Y.train <- PH2[initsplit3,]


#Create testing data to evaluate the model
X.test <- predictors[-initsplit3,]
Y.test <- PH2[-initsplit3,]

```



### Random forest

```{r}
rfModel <- randomForest(X.train, Y.train, importance=TRUE, ntree = 500)

```


### Cubist

```{r}
set.seed(100)


cubistTune <- train(X.train, Y.train,
    method = "cubist",
    verbose = FALSE)

```


### Gradient boosting

```{r}
gbmGrid <- expand.grid(
         interaction.depth = seq(1, 7, by = 2),
         n.trees = seq(100, 1000, by = 50),
         shrinkage = c(0.01, 0.1),
         n.minobsinnode = 10
         )

set.seed(100)

gbmTune <- train(X.train, Y.train,
    method = "gbm",
    tuneGrid = gbmGrid,
    verbose = FALSE)

```

Tree Predictions:

```{r}
set.seed(150)


randomPred <- predict(rfModel, newdata = X.test)
randomResample <- postResample(pred=randomPred, obs = Y.test)

gbmPred <-predict(gbmTune, newdata=X.test)
gbmResample <- postResample(pred=gbmPred, obs=Y.test)

cubistPred <-predict(cubistTune, newdata=X.test)
cubistSample <- postResample(pred=cubistPred, obs = Y.test)

```

```{r}
display <- rbind(
"Random Forest" = randomResample,
"Gradient Boosted Tree" = gbmResample,
"Cubist" = cubistSample
)


display %>% kable() %>% kable_paper()


```

Importance of Variables (Best Tree Model):

Based on the same criteria from above, we can see much better results from our tree models. The Cubist and Random Forest produce similar results with a slight edge for Cubist. 

```{r}
plot(varImp(cubistTune), top = 10)
```

This model places equal importance to `Mnf.Flow` and `Alch.Rel`. The other top predictors are `Carb.Rel`, `Pressure.Vacuum`, and `Temperature`. From a chemical standpoint, it makes sense that the model would place a high importance in predicting PH to `Alch.Rel` and `Carb.Rel`, since those variables deal with oxygen and hydrogen (which either increase or decrease PH). 


# Model Selection

# Further Evaluation of Most Important Variables

# Conclusion
