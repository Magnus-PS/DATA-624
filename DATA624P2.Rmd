---
title: "DATA624HW8"
author: "Gabriel Abreu, Magnus Skonberg"
date: "`r Sys.Date()`"
output: 
 html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: flatly
    highlight: tango
---

```{r setup, include = FALSE }

# Libraries and Options

knitr::opts_chunk$set(echo = F, warning = F, message = F, eval = T, 
                      fig.height = 5, fig.width = 10) 

library(RCurl)
library(dplyr)
library(forecast)
library(ggplot2)
library(knitr)
library(inspectdf)
library(corrplot)
library(tidyverse)
library(VIM)
library(tidyr)
library(kableExtra)
library(mice)
library(Boruta)

# Comment out below if / when we need

# library(MASS)
# library(mlbench)
# library(caret)
# library(earth)
# library(nnet)
# library(car)
# library(AER)
# library(faraway)
# library(vcd)
# library(caret)
# library(boot)
# library(pscl) #predict.zeroinfl

```

```{r custom-functions, include=FALSE}

options(scipen = 9)
set.seed(123)

boxplot_depend_vs_independ <- function(df_train, target_name) {

  train_int_names <- df_train %>% select_if(is.numeric)
  int_names <- names(train_int_names)
  myGlist <- vector('list', length(int_names))
  names(myGlist) <- int_names
  
  for (i in int_names) {       
 
   myGlist[[i]] <- 
       ggplot(df_train, aes_string(x = target_name, y = i)) + 
        geom_boxplot(color = 'steelblue', outlier.color = 'firebrick', 
                     outlier.alpha = 0.35) +
        labs(title = paste0(i,' vs target'), y = i, x= 'target') +
        theme_minimal() + 
        theme(
          plot.title = element_text(hjust = 0.45),
          panel.grid.major.y =  element_line(color = "grey", 
                                             linetype = "dashed"),
          panel.grid.major.x = element_blank(),
          panel.grid.minor.y = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.ticks.x = element_line(color = "grey")
        )
       
      }

    myGlist <- within(myGlist, rm(target_name))
    gridExtra::grid.arrange(grobs = myGlist, ncol = 4)
}

plot_corr_matrix <- function(dataframe, significance_threshold){
  title <- paste0('Correlation Matrix for significance > ',
                  significance_threshold)
  
  df_cor <- dataframe %>% mutate_if(is.character, as.factor)
  
  df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)
  #run a correlation and drop the insignificant ones
  corr <- cor(df_cor)
  #prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  corr[corr == 1] <- NA 
  #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr) 
  #select significant values  
  corr <- subset(corr, abs(Freq) > significance_threshold) 
  #sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),] 
  #print table
  # print(corr)
  #turn corr back into matrix in order to plot with corrplot
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
  
  #plot correlations visually
  corrplot(mtx_corr,
           title=title,
           mar=c(0,0,1,0),
           method='color', 
           tl.col="black", 
           na.label= " ",
           addCoef.col = 'black',
           number.cex = .9)
}
```

# Background

The purpose of our second project is to work as a team to apply concepts from the 2nd half of our Predictive Analytics course to a beverage data set. More specifically, to explore the data, determine whether we might use a linear regression, non-linear regression or tree-based model, to then build, compare, select our optimal model, and support why we made the selection that we did.

## ABC Beverage Spec
 
**This is role playing. I am your new boss. I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me. My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.**

**Please use the historical data set I am providing. Build and report the factors in BOTH a technical and non-technical report. I like to use Word and Excel. Please provide your non-technical report in a  business friendly readable document and your predictions in an Excel readable format. The technical report should show clearly the models you tested and how you selected your final approach.**

```{r import-data, message=FALSE, warning=FALSE, eval=TRUE}

# read in data (as CSVs)
train <- read.csv("https://raw.githubusercontent.com/Magnus-PS/DATA-624/main/624P2TrainData.csv")
test <- read.csv("https://raw.githubusercontent.com/Magnus-PS/DATA-624/main/624P2EvalData.csv")

#rename i..Brand.Code
names(train)[names(train) == "ï..Brand.Code"] <- "Brand.Code"
names(test)[names(test) == "ï..Brand.Code"] <- "Brand.Code"

# label datasets
train$Dataset <- 'train'
test$Dataset <- 'test'

# merge datasets
final_df <- rbind(train, test)

```

## Approach

We will explore, analyze and model a data set containing approximately 2,600 training and 300 test records representing various brands of a beverage. The variables appear to be related to beverage chemical and manufacturing properties as well as the brand of beverage created. 

The response variable is `PH` which represents the pH level of each beverage. We'll generate multiple models and then select the best performing model for our training data prior to casting predictions for our test data's `PH` variable (which is currently all NAs).

We'll follow the general, high-level approach that is common in Data Science:

* Exploratory Data Analysis (EDA),
* Data Preparation,
* Model Building, and
* Model Selection

Prior to casting our predictions.

---

# Exploratory Data Analysis (EDA)

The purpose of exploring our data is to enhance the precision of the questions we’re asking while building a firm understanding of the data. The aim is to familiarize ourselves with the data's structure, the status of missing values, outliers, predictive strength, and correlation to then take the actions necessary to prepare our data for model building. 

We get to know the structure, value ranges, proportion of missing values, distribution of values and correlation with the target variable. After this point, we should have enough insight to prepare our data and build our model(s).

## Data Structure

We utilize the built-in `glimpse` method to gain insight into dimensions, variable characteristics, and value ranges for our training data:

```{r glimpse-data}

glimpse(train)

```

From above, we gather that:

* we're dealing with training data with 33 variables (discluding our addition of a "dataset" variable) and 2571 observations,
* a response variable `pH` of type double,
* a `Brand Code` categorical variable,
* remaining dependent variables of type int and double,
* variables that may be useless (ie. `Hyd.Pressure1` with all 0s),
* a significant difference in the scale of many features (ie. `Filler.Speed` v. `Oxygen.Filler`), and
* a presence of missing / NA values.

Let's get a high level look at our distributions: 

```{r}

summary(train)

```

We note presence of negative values (ie. `Pressure.Vacuum`), difference of scales and distribution, and confirm the presence of NA values (including 212 values in `MFR`) across numerous variables.

## Missing Values

Next, we investigate missing values in greater depth:

```{r aggr-plots1, results=F, fig.height=8, fig.width=15}

VIM::aggr(final_df %>% filter(Dataset == 'train'), col=c('green','red'), numbers=T, sortVars=T,
          cex.axis = .7,
          ylab=c("Proportion of Data", "Combinations and Percentiles"))

```

From the proportion chart above on the left we can see that:

* `MFR` has ~8% missing values (the most of any variable),
* a significant proportion of variables have 1-2% missing values (ie. `Filler.Speed`),
* a significant proportion of variables have less than 1% missing data (ie. `PSC.Fill`), and
* 4 variables (ie. `Brand.Code`) have no missing data.

When we shift our attention to the Combinations and Percentiles chart (on the right), it appears that while there may be instances of related patterns between variables (ie. `MFR` and `Fill.Pressure`), the data is primarily missing at random.

## Data Type Conversions

We're primarily dealing with numeric variables but in order to properly utilize our categorical variable, we convert it to type factor. We re-label empty strings as "Unlabelled" and then convert our `Brand.Code` variable to be of type factor:

```{r}

#Relabel missing brand codes to be "Unlabelled"
final_df$Brand.Code[final_df$Brand.Code == ""] <- NA
final_df$Brand.Code <- final_df$Brand.Code %>% tidyr::replace_na('Unlabelled')

#convert brand code to factor
final_df  <- final_df %>% dplyr::mutate(Brand.Code = factor(Brand.Code, levels = c('Unlabelled', 'A','B','C','D'), ordered = TRUE))

```

## Numeric Variable Distributions

Earlier we'd noted the vast difference in ranges between numeric variables. To explore this point further and gain greater insight as to the distribution of each variable, we utilize inspectdf's `inspect_num()` function:

```{r, fig.height=20, fig.width=15}

#Variable distributions
inspectdf::inspect_num(final_df %>% filter(Dataset == 'train')) %>% 
  show_plot()

```

We observe a range of distributions:

* `Carb.Pressure`, `Carb.Temp`, `Carb,Volume`, `Fill.Ounces`, `PC.Volume`, `PH`, `Pressure.Vacuum`, `PSC`, `PSC.Fill`, and `Temperature` have relatively normal spreads where just a few of the variables are skewed and would require centering / normalization,
* `Alch.Rel`, `Bowl.Setpoint`, and `Pressure.Setpoint` appear to resemble distinct numerical variables with distributions centered about a few values, and
* remaining distributions are non-normal and in need of scaling and centering.

From above, there may be a place for introducing dummy / flag variables (ie. `Alch.Rel` = 0.6) and that
centering and scaling will prove essential prior to model-building. Fortunately, R has built in functions to center and scale automatically and en masse.

## Categoric Contingency Table

Next, we create a contingency table for our categorical variable to better understand the relationship between `Brand.Code` and `PH`:

```{r}

table(final_df$Brand.Code, final_df$PH)

```

From above we might extend that:

* B is the most popular beverage brand,
* that the distributons are quite sparse outside of the range of 8.3 thru 8.8, and 
* the brand of beverage appears to have some correlation to PH level.

## Boxplots

With a basic understanding of the distribution of each of our features, we turn to the boxplot of each numeric distribution in order to visualize the magnitude of outliers:

```{r boxplot2, fig.height=20, fig.width=12} 

# Utilize customized box plot generation function
df_train <- final_df %>% filter(Dataset == 'train')
boxplot_depend_vs_independ(df_train, df_train$PH)

```

What we gather from the output is:

* our response variable (`PH`) has a range of ~8.4-8.7 with outlier values > 9.0 and ~ 8.0,
* `Mnf.Flow`, `Hyd.Pressure2`, `Hyd.Pressure3` `Usage.cont`, `Carb.Flow`, `Density`, `Balling`, `Bowl.Setpoint`, `Pressure.Setpoint`, and `Balling.Lvl` have **no outliers**,
* `Carb.Volume`, `Carb.Pressure`, `PSC.CO2`, `Hyd.Pressure1`, `Filler.Level`, `Pressure.Vacuum`, and `Alch.Rel` have **minimal outliers**, and
* remaining (15) variables have a **significant presence of outliers**.

As such, outlier-handling will be essential in properly predicting our continuous, numeric response variable (`PH`).

## Correlation Matrix

Having reviewed the structure, distributions, and presence of outliers for our variables, we turn our attention to exploring the relationship these variables have with one another via **correlation matrix**. We consider only variables with a correlation significant > 0.1 in our plot:

```{r correlation-matrix , warning=FALSE}

#Utilize custom-built correlation matrix generation function
plot_corr_matrix(final_df %>% filter(Dataset == 'train'), -1)

```

It does not appear that multicollinearity is a concern of ours. From this we might extend that feature exclusion based on multicollinearity will not be a concern.

## Variable Importance

As a final step in the exploration of our data, we shoot to understand which variables have the strongest v. weakest relationship with `PH` level. This information may prove useful later during feature exclusion / selection:

```{r boruta, comment=FALSE, warning=FALSE, message=FALSE, fig.height = 8, fig.width = 10}

# Perform Boruta search
boruta_output <- Boruta(PH ~ ., data=na.omit(train), doTrace=0, maxRuns = 1000)

# Get significant variables including tentatives
boruta_signif <- getSelectedAttributes(boruta_output, withTentative = TRUE)
#print(boruta_signif)

# Plot variable importance
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")

```

Our Boruta output indicates that 4 of our features may be insignificant (`PSC.CO2`, `PSC`, `PSC.Fill` and `Carb.Temp`) while the remainder appear to be significant. With `Mnf.Flow` carrying the greatest variable importance (by a narrow margin).

## EDA Summary

The training dataset has 33 variables and 2571 observations. The remediation actions became apparent over the course of our exploratory data analysis:

* There are numerous features with a weak relationship to `PH`. While on the surface we might consider dropping these variables (ie. `PSC.CO2`), first we should see whether their importance increases once we consider other features during our regression / model-building.
* Multicollinearity does not appear to be a concern.
* There are outliers in the majority of our features that will need to be addressed.
* Imputing missing values will be an important factor in our modeling. From our analysis, KNN may be a logical approach since beverages of similar compositions (PH levels) should have similar values.
* Centering and scaling will likely be essential prior to model-building and especially depending on the approach we elect for predicting our continuous, numeric response variable (`PH`).

The recommendations above provide a “starting line” for our data preparation. Being that, the majority of our features appear to have a relatively strong relationship with `PH`, we'd anticipate being able to pull much signal from the variables at hand and developing a strong model.

---

# Data Prep

With insights from exploratory data analysis, we set out to prepare our data for modeling. Our aim is to "optimize" our data for modeling. We'll impute missing values, identify and rectify outliers, and then proceed to model building.

## Missing Value Imputation

Since there are many variables at play, we took a nearest neighbor approach (consider the 5 nearest neighbors) for imputing values in columns where missing values were identified prior to verifying the presence of 0 NA values in our output:

```{r imputation}

#kNN imputation
#summary(train) #identify variables with missing values

final_df <- VIM::kNN(final_df, variable = c('Carb.Volume','Fill.Ounces', 'PC.Volume', 'Carb.Pressure',  'Carb.Temp', 'PSC', 'PSC.Fill', 'PSC.CO2', 'Mnf.Flow', 'Carb.Pressure1', 'Fill.Pressure', 'Hyd.Pressure1', 'Hyd.Pressure2', 'Hyd.Pressure3', 'Hyd.Pressure4', 'Filler.Level', 'Filler.Speed', 'Temperature', 'Usage.cont', 'Carb.Flow', 'Density', 'MFR', 'Balling', 'Pressure.Vacuum', 'Oxygen.Filler', 'Bowl.Setpoint', 'Pressure.Setpoint', 'Air.Pressurer', 'Alch.Rel', 'Carb.Rel', 'Balling.Lvl'), k = 5)

# verify successful imputation
colSums(is.na(final_df))

```

We confirm successful imputation via presence of all 0s when we search our variables for NA values. *Note: the `PH` column has 271 NA values because we imputed training and test data in one fell swoop and didn't touch this column since this is what we're later predicting.*

## Outlier Handling

With missing values imputed, we proceed to handle those that can skew the predictive capabilities of our models, the outliers.

**LEFT OFF HERE**

NOTE TO GABE: I didn't go beyond this point

for outlier handling we could build a basic linear model and use Cook's distance ...

```{r}

```

## Normalization

We can center and scale our data so that it all sits on the same scale with normal distribution. If I recall correctly this is important for regression but may not be so much so / may automatically be handled with models like RF ...

# Model Building

## Multi-linear regression

AIC optimized (stepAIC function) v. kitchen sink (all variable) models to compare accuracy.

## Random forest

Optimized parameter selection v. out-of-the-box

## Gradient boosting


...

I don't believe Neural Networks would be useful here being that our dataset's not that large ... Maybe instead of considering another model we play with parameters within models to optimize and then compare best Multi-linear regression v. Random forest v. Gradient boosting ...

# Model Selection

# Conclusion
