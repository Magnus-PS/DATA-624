---
title: "DATA624Pres"
author: "Gabriel Abreu, Magnus Skonberg"
date: "`r Sys.Date()`"
output: 
 html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: flatly
    highlight: tango
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

```

```{r library, comment=FALSE, warning=FALSE, message=FALSE }

library(dplyr)
library(forecast)
library(tidyverse)
library(randomForest)
library(tsibble)
library(readr)

```

## Background

The purpose of this presentation is to explore a real world application of a **Random Forest** model on time series data. Random forest is one of the simplest, most robust, powerful and popular ML algorithms and we wanted to go "full circle" in exploring this popular algorithm, from a subsection of this week's tree-based model topic, on time series data (something we covered in-depth earlier in the semester).

### Approach

In completing this exploration of a Random Forest model we:

* pre-process and visualize our data upon reading it in,
* difference and transform the data so as to make it stationary,
* utilize time delay embedding to bring our data into a form an ML model can handle,
* train our model, consider its statistics, and compare its performance with that of a SNAIVE model.

### Data

We use a time series from the German Statistical Office on German wage and income tax revenue from 1999 â€“ 2019 (after tax redistribution). *Note: we end up dropping entries from 2019 because they aren't complete for the year.*

The can be downloaded from [STATWORX's GitHub](https://github.com/STATWORX/blog/tree/master/time%20series%20forecasting).

---

## Pre-process and Visualize

In order to use random forest for time series data we transform, difference, and embed our data.

We read in and pre-process the German tax data so as to *transform* our csv file into a compatible time series format:

```{r}

urlfile="https://raw.githubusercontent.com/Magnus-PS/DATA-624/main/tax.csv"
#mydata<-read_csv(url(urlfile))
#head(mydata)

# read in the csv file
tax_tbl <- readr::read_delim(file = urlfile, 
                             delim = ";", #split on ;
                             col_names = c("Year", "Type", month.abb), #label columns
                             skip = 1, #skip 1st row
                             na = c("...")) %>% #label ... entries as NA
    select(-Type) %>% #drop tax type label
    gather(Date, Value, -Year) %>% #unite date and value minus year
    unite("Date", c(Date, Year), sep = " ") %>% #combine month and year
    mutate( Date = Date %>% 
            lubridate::parse_date_time("m y") %>% 
            yearmonth()) %>% 
    drop_na() %>% #drop NAs
    as_tsibble(index = "Date") %>% #specify Date column as index
    filter(Date <= yearmonth(as.Date("2018-12-01"))) #drop early 2019 data

# convert to ts format
tax_ts <- as.ts(tax_tbl)

```

To do so we drop the "Type" column, exclude data from 2019, combine month and year into one column, and convert our data from tabular to time series format.

Next, we verify that there are no missing data issues and proceed to visualizing German Wage and Income Tax from 1999 to 2018:

```{r}

#Explore missing data
#has_gaps(tax_tbl) # implicit missings
#colSums(is.na(tax_tbl[, "Value"])) # explicit missings

#Visualize German Tax Data
plot_org <- tax_tbl %>% 
  ggplot(aes(Date, Value / 1000)) + # to get the axis on a more manageable scale
  geom_line() +
  theme_minimal() +
  labs(title = "German Wage and Income Taxes 1999 - 2018", x = "Year", y = "Euros")

plot_org

```

We observe seasonality and trend in our data. German tax income, likely similar to American tax incomes, appears to have a major spike once a year (likely when personal taxes are due) with multiple smaller spikes throughout the year. Additionally, the total tax income is climbing on a year-to-year basis.

## Data Preparation

*What does this mean?*

We have to difference our data to make our non-stationary time series stationary and remove the variance within our data that is present from level to level. This, differencing time series data to make it stationary, is an essential step for classical time series models much like it will be for our random forest model. 

**Reminder:** making our data stationary means that the mean and variance of the series is stable and does not change over time. It implies stability.

### Difference and Transform

We apply first order differencing, the difference between consecutive obervations (ie. one month to the next), to stabilize the mean of our time series and a log transformation to stabilize the variance. The prior plot is set above our transformed plot to highlight the difference:

```{r}

# pretend we're in December 2017 and have to forecast the next twelve months
tax_ts_org <- window(tax_ts, end = c(2017, 12))

# log transform and difference the data
tax_ts_trf <- tax_ts_org %>% 
  log() %>% 
  diff(nsdiffs(tax_ts_org)) #required order of differencing: 1

# check out the difference! (pun)
plot_trf <- tax_ts_trf %>% 
  autoplot() +
  xlab("Year") +
  ylab("Euros") +
  ggtitle("German Wage and Income Taxes 1999 - 2018") +
  theme_minimal()

gridExtra::grid.arrange(plot_org, plot_trf)

```

It appears that first order differencing and log transformation have set our data straight. We proceed with stationary data.

### Time Delay Embedding

From here, we'll train our random forest model and make forecasts for 2018, which we'll later compare to the actual values to assess the accuracy of our model. In order to do so, we'll later have to reverse our transformations to bring them to the original scale.

Before doing so we have to bring our data into a form that a machine learning algorithm can handle. What we need to do, essentially, is transform a vector into a matrix (a structure that a ML algorithm can work with).

To do so, we make use of a concept called *time delay embedding* using the `embed()` function:

```{r}

lag_order <- 6 # desired lag number (6 months)
horizon <- 12 # forecast duration (12 months)

tax_ts_mbd <- embed(tax_ts_trf, lag_order + 1) # embedding magic!

```

Time delay embedding represents a time series in a Euclidean space (a 3-D geometrical space) and allows us to proceed with any linear or non-linear regression method on time series data, whether Random Forest, Gradient Boosting, etc. We set our desired number of lags to be 6 months and the duration of our forecast to be 12 months.

## Model Building

We make use of a [direct forecasting strategy](https://insightr.wordpress.com/2018/01/10/direct-forecast-x-recursive-forecast/), split our data into training and test sets, and fit our "out of the box" random forest model:

```{r}

#train-test  split
y_train <- tax_ts_mbd[, 1] # the target
X_train <- tax_ts_mbd[, -1] # everything but the target

y_test <- window(tax_ts, start = c(2018, 1), end = c(2018, 12)) # 2018
X_test <- tax_ts_mbd[nrow(tax_ts_mbd), c(1:lag_order)] # test set consisting of 6 most recent values (6 lags) of training set

#forecast
forecasts_rf <- numeric(horizon)

for (i in 1:horizon){
  # set seed
  set.seed(333)

  # fit the model
  fit_rf <- randomForest(X_train, y_train)

  # predict using the test set
  forecasts_rf[i] <- predict(fit_rf, X_test)

  # here is where we repeatedly reshape the training data to reflect the time distance
  # corresponding to the current forecast horizon.
  y_train <- y_train[-1] 

  X_train <- X_train[-nrow(X_train), ] 
}

```

We train 12 models and get 12 separate forecasts (one for 1 mo, one for 2mos, etc etc. up until 12 months) and before we can assess our Random Forest model, we have to transform the forecasts back to the original scale:

```{r}

# calculate the exp term
exp_term <- exp(cumsum(forecasts_rf))

# extract the last observation from the time series (y_t)
last_observation <- as.vector(tail(tax_ts_org, 1))

# calculate the final predictions
backtransformed_forecasts <- last_observation * exp_term

# convert to ts format
y_pred <- ts(
  backtransformed_forecasts,
  start = c(2018, 1),
  frequency = 12
)

# add the forecasts to the original tibble
tax_tbl <- tax_tbl %>% 
  mutate(Forecast = c(rep(NA, length(tax_ts_org)), y_pred))

# visualize the forecasts
plot_fc <- tax_tbl %>% 
  ggplot(aes(x = Date)) +
  geom_line(aes(y = Value / 1000)) +
  geom_line(aes(y = Forecast / 1000), color = "blue") +
  theme_minimal() +
  labs(
    title = "Forecast of the German Wage and Income Tax for the Year 2018",
    x = "Year",
    y = "Euros"
  )

plot_fc
accuracy(y_pred, y_test)

```

We reverse the effects of earlier differencing and (log) transformation by exponentiating the cumulative sum of our transformed forecasts (reverse log) and multiplying the result with the last observation of our time series (reverse diff) ... the resulting plot and output statistics hold some promise.

Based on the plot, our forecast appears to capture the trend and seasonality of the data.

When we consider accuracy metrics, on the surface it appears that we have a high RMSE but when we consider that we'd divided our tax value by 1000 when plotting it brings the high RMSE value into perspective. We're dealing with large tax revenue values and so a larger RMSE is to be expected. Thus we put more of an emphasis on the MAPE. The MAPE (2.64%) is excellent. Especially when we consider that this is performance was with respect to unseen test data.

For comparison, we visit the performance statistics of a Seasonal Naive model:

```{r}

snaive <- forecast(snaive(tax_ts_org), h = horizon)

tax_ts %>% 
  autoplot() +
  autolayer(snaive, PI = FALSE)

accuracy(snaive, y_test)

```

The plot looks decent although it doesn't quite capture the upward trend in the same way our random forest model did. When we compare accuracy statistics, we see that our Random Forest model had a far lower RMSE value with a MAPE that was half the value of our seasonal naive model's performance.

Our Random Forest model was a far better model than a seasonal naive model in predicting German wage and taxes for 2018.

## Conclusion

**It appears that this was a good real world application of a Random Forest model.** We were able to predict German wage and income taxes for a full year with less than 3% mean absolute percent error (MAPE).

How might it be further improved?

1. tune hyper parameters,
2. experiment with box cox rather than log transformation,
3. compare to a boosting model (ie. AdaBoost or XGBoost)

---

## Reference

This presentation was made with reference to the following resource(s):

* Manuel Tilgner. (2019). **Time Series Forecasting with Random Forest**. Retrieved from: https://www.statworx.com/at/blog/time-series-forecasting-with-random-forest/
